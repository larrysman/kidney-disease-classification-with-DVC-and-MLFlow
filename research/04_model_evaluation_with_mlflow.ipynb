{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3666cf3",
   "metadata": {},
   "source": [
    "##### MDOEL EVALUATION AND MLFLOW INTEGRATION TO TRACK EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6008d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Working Directory:c:\\Users\\Olanrewaju Adegoke\\DLOPs\\Kidney_Disease_Classification\\research\n",
      "Current Working Directory after Changed: c:\\Users\\Olanrewaju Adegoke\\DLOPs\\Kidney_Disease_Classification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Original Working Directory:{os.getcwd()}\")\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "#print(f\"Current Working Directory after Changed: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e12ab",
   "metadata": {},
   "source": [
    "###### GET DETAILS BELOW FROM THE DAGSHUB.COM AFTER YOU HAVE CREATED A NEW PROJECT AND LINK WITH YOUR GITHUB\n",
    "\n",
    "`os.environ[\"MLFLOW_TRACKING_URL\"]=\"https://dagshub.com/USER_NAME/PROJECT_TITLE.mlflow\"`\n",
    "\n",
    "`os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"USER_NAME\"`\n",
    "\n",
    "`os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"40_HASH_TOKENS\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc9d3cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Functional name=functional, built=True>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD THE MODEL\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"artifacts/training/trained_model.h5\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE ENTITY - AFTER THE CONFIG AND PARAMS ARE UPDATED\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    trained_model_path: Path\n",
    "    training_data: Path\n",
    "    scores_file_path: Path\n",
    "    all_params: dict\n",
    "    #mlflow_url: str\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "    params_epochs: int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d45719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE CONFIGURATION INSIDE THE SRC/CONFIG - THIS IS ESSENTIAL AS THE CURRENT WORKFLOW WILL USE IT TO RUN\n",
    "\n",
    "from src.Kidney_classifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "\n",
    "from Kidney_classifier.utils.common import read_yaml_file, create_directories, save_json_file\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml_file(config_filepath)\n",
    "        self.params = read_yaml_file(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    \n",
    "    # def get_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \n",
    "    #     eval_config = ModelEvaluationConfig(\n",
    "    #         trained_model_path=\"artifacts/training/trained_model.h5\",\n",
    "    #         training_data=\"artifacts/data_ingestion/kidney_disease_scan_image\",\n",
    "    #         mlflow_url=\"https://dagshub.com/USER_NAME/PROJECT_TITLE.mlflow\", # change this to the actual repo name\n",
    "    #         all_params = self.params.prepare_base_model,\n",
    "    #         params_augmentation=self.params.prepare_base_model.AUGMENTATION,\n",
    "    #         params_image_size=self.params.prepare_base_model.IMAGE_SIZE,\n",
    "    #         params_batch_size=self.params.prepare_base_model.BATCH_SIZE,\n",
    "    #         params_epochs=self.params.prepare_base_model.EPOCHS\n",
    "    #     )\n",
    "\n",
    "    #     return eval_config\n",
    "    \n",
    "    \n",
    "    def get_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        all_eval_config = self.config.model_evaluation\n",
    "        all_eval_params = self.params.prepare_base_model\n",
    "\n",
    "        eval_config = ModelEvaluationConfig(\n",
    "            trained_model_path=Path(all_eval_config.trained_model_path),\n",
    "            training_data=Path(all_eval_config.training_data),\n",
    "            scores_file_path=Path(all_eval_config.scores_file_path),\n",
    "            all_params = self.params.prepare_base_model,\n",
    "            #mlflow_url=all_eval_config.mlflow_url,\n",
    "            params_augmentation=all_eval_params.AUGMENTATION,\n",
    "            params_image_size=all_eval_params.IMAGE_SIZE,\n",
    "            params_batch_size=all_eval_params.BATCH_SIZE,\n",
    "            params_epochs=all_eval_params.EPOCHS\n",
    "        )\n",
    "\n",
    "        return eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "344312bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE COMPONENTS - THIS IS WHERE YOU CREATE THE FILE FOR EVALUATING THE MODEL\n",
    "\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "from Kidney_classifier import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf2bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def _testing_validation_generator(self):\n",
    "\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale = 1./255,\n",
    "            validation_split=0.10\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        validation_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        self.validation_generator = validation_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_trained_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    \n",
    "\n",
    "    def model_evaluation(self):\n",
    "        self.model = self.load_trained_model(self.config.trained_model_path)\n",
    "        self._testing_validation_generator()\n",
    "        self.score = model.evaluate(self.validation_generator)\n",
    "        self.save_score()\n",
    "\n",
    "    def save_score(self):\n",
    "        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
    "        #save_json_file(path=Path(\"scores.json\"), data=scores)\n",
    "        self.config.scores_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        save_json_file(path=self.config.scores_file_path, data=scores)\n",
    "\n",
    "\n",
    "    \n",
    "    # MLFLOW EXPERIMENT TRACKING CODE SNIPET\n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_url)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(\n",
    "                {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
    "            )\n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.keras.log_model(self.model, \"model\", registered_model_name=\"VGG16Model_as_base_transfer_learning\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(self.model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4117301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Olanrewaju Adegoke\\DLOPs\\Kidney_Disease_Classification\\KIDNEY_CLASS_VENV\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 0.9750 - loss: 0.2282\n"
     ]
    }
   ],
   "source": [
    "# UPDATE THE PIPELINE\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.model_evaluation()\n",
    "    #model_evaluation.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f2ba4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # CODE REFACTORED FOR MODEL EVALUATION AND MLFLOW\\n\\n# MODEL EVALUATION\\n\\nclass ModelEvaluation2:\\n    def __init__(self, config: ModelEvaluationConfig):\\n        self.config = config\\n        self.model = None\\n        self.score = None\\n        self.validation_generator = None\\n\\n    def _testing_validation_generator(self):\\n        datagenerator_kwargs = dict(\\n            rescale=1./255,\\n            validation_split=0.10\\n        )\\n\\n        dataflow_kwargs = dict(\\n            target_size=self.config.params_image_size[:-1],\\n            batch_size=self.config.params_batch_size,\\n            interpolation=\"bilinear\"\\n        )\\n\\n        validation_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(**datagenerator_kwargs)\\n\\n        self.validation_generator = validation_datagenerator.flow_from_directory(\\n            directory=self.config.training_data,\\n            subset=\"validation\",\\n            shuffle=False,\\n            **dataflow_kwargs\\n        )\\n\\n    @staticmethod\\n    def load_trained_model(path: Path) -> tf.keras.Model:\\n        return tf.keras.models.load_model(path)\\n\\n    def model_evaluate(self):\\n\\n        self.model = self.load_trained_model(self.config.trained_model_path)\\n        self._testing_validation_generator()\\n        self.score = self.model.evaluate(self.validation_generator)\\n        self.save_score()\\n        return self.model, self.score\\n\\n    def save_score(self):\\n        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\\n        #save_json_file(path=Path(\"scores.json\"), data=scores)\\n        self.config.scores_file_path.parent.mkdir(parents=True, exist_ok=True)\\n        save_json_file(path=self.config.scores_file_path, data=scores)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # CODE REFACTORED FOR MODEL EVALUATION AND MLFLOW\n",
    "\n",
    "# MODEL EVALUATION\n",
    "\n",
    "class ModelEvaluation2:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.score = None\n",
    "        self.validation_generator = None\n",
    "\n",
    "    def _testing_validation_generator(self):\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.10\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        validation_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(**datagenerator_kwargs)\n",
    "\n",
    "        self.validation_generator = validation_datagenerator.flow_from_directory(\n",
    "            directory=self.config.training_data,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_trained_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "\n",
    "    def model_evaluate(self):\n",
    "        \n",
    "        self.model = self.load_trained_model(self.config.trained_model_path)\n",
    "        self._testing_validation_generator()\n",
    "        self.score = self.model.evaluate(self.validation_generator)\n",
    "        self.save_score()\n",
    "        return self.model, self.score\n",
    "\n",
    "    def save_score(self):\n",
    "        scores = {\"loss\": self.score[0], \"accuracy\": self.score[1]}\n",
    "        #save_json_file(path=Path(\"scores.json\"), data=scores)\n",
    "        self.config.scores_file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        save_json_file(path=self.config.scores_file_path, data=scores)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9120a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # UPDATE MLFLOW \\n\\nimport mlflow\\nimport mlflow.keras\\nfrom urllib.parse import urlparse\\n\\nclass MLflowLogger:\\n    def __init__(self, config):\\n        self.config = config\\n\\n    def log_metrics_and_model(self, model, score):\\n\\n        \"Logs parameters, metrics, and model into MLflow.\"\\n\\n        mlflow.set_registry_uri(self.config.mlflow_url)\\n        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\\n\\n        with mlflow.start_run():\\n            # Logging hyperparameters\\n            mlflow.log_params(self.config.all_params)\\n\\n            # Logging evaluation metrics\\n            mlflow.log_metrics({\"loss\": score[0], \"accuracy\": score[1]})\\n\\n            # Logging and optionally register the model\\n            if tracking_url_type_store != \"file\":\\n                mlflow.keras.log_model(\\n                    model,\\n                    \"model\",\\n                    registered_model_name=\"VGG16Model_as_base_transfer_learning\"\\n                )\\n            else:\\n                mlflow.keras.log_model(model, \"model\")\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # UPDATE MLFLOW \n",
    "\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class MLflowLogger:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def log_metrics_and_model(self, model, score):\n",
    "        \n",
    "        \"Logs parameters, metrics, and model into MLflow.\"\n",
    "        \n",
    "        mlflow.set_registry_uri(self.config.mlflow_url)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            # Logging hyperparameters\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "\n",
    "            # Logging evaluation metrics\n",
    "            mlflow.log_metrics({\"loss\": score[0], \"accuracy\": score[1]})\n",
    "\n",
    "            # Logging and optionally register the model\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.keras.log_model(\n",
    "                    model,\n",
    "                    \"model\",\n",
    "                    registered_model_name=\"VGG16Model_as_base_transfer_learning\"\n",
    "                )\n",
    "            else:\n",
    "                mlflow.keras.log_model(model, \"model\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99b33c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Assuming ModelEvaluationConfig is defined somewhere\\nconfig = ModelEvaluationConfig()\\n\\n# Step 1 — Evaluate model\\nevaluator = ModelEvaluation2(config)\\nmodel, score = evaluator.evaluate()\\n\\n# Step 2 — Log results to MLflow\\nmlflow_logger = MLflowLogger(config)\\nmlflow_logger.log_metrics_and_model(model, score)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Assuming ModelEvaluationConfig is defined somewhere\n",
    "config = ModelEvaluationConfig()\n",
    "\n",
    "# Step 1 — Evaluate model\n",
    "evaluator = ModelEvaluation2(config)\n",
    "model, score = evaluator.evaluate()\n",
    "\n",
    "# Step 2 — Log results to MLflow\n",
    "mlflow_logger = MLflowLogger(config)\n",
    "mlflow_logger.log_metrics_and_model(model, score)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40c1c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # UPDATE PIPELINE\\n\\ntry:\\n    # Step 1: Load all configurations\\n    config = ConfigurationManager()\\n    model_evaluation_config = config.get_evaluation_config()\\n\\n    # Step 2: Evaluate model\\n    model_evaluator = ModelEvaluation2(config=model_evaluation_config)\\n    model, score = model_evaluator.evaluate()\\n\\n    # Step 3: Log results to MLflow\\n    mlflow_logger = MLflowLogger(config=model_evaluation_config)\\n    mlflow_logger.log_metrics_and_model(model, score)\\n\\nexcept Exception as e:\\n    raise e\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # UPDATE PIPELINE\n",
    "\n",
    "try:\n",
    "    # Step 1: Load all configurations\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_evaluation_config()\n",
    "\n",
    "    # Step 2: Evaluate model\n",
    "    model_evaluator = ModelEvaluation2(config=model_evaluation_config)\n",
    "    model, score = model_evaluator.evaluate()\n",
    "\n",
    "    # Step 3: Log results to MLflow\n",
    "    mlflow_logger = MLflowLogger(config=model_evaluation_config)\n",
    "    mlflow_logger.log_metrics_and_model(model, score)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KIDNEY_CLASS_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
